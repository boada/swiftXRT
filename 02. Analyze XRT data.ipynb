{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from glob import glob\n",
    "#import aplpy\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve\n",
    "from astropy.convolution import Gaussian2DKernel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add extra code to run things in parallel.\n",
    "The combining exposure maps takes a REALLY long time. So I added this bit so I can run it all in parallel.\n",
    "\n",
    "You don't need to run any of this if you don't want to run things in paralle. I'll try to comment out code at the bottom so you can run things in parallel or not depending on what you want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import traceback, functools\n",
    "\n",
    "def error(msg, *args):\n",
    "    multiprocessing.log_to_stderr()\n",
    "    return multiprocessing.get_logger().error(msg, *args)\n",
    "\n",
    "def trace_unhandled_exceptions(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapped_func(*args, **kwargs):\n",
    "        try:\n",
    "            func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            error(traceback.format_exc())\n",
    "            raise\n",
    "\n",
    "    return wrapped_func\n",
    "\n",
    "class AsyncFactory:\n",
    "    def __init__(self, func, cb_func):\n",
    "        self.func = func\n",
    "        self.cb_func = cb_func\n",
    "        self.pool = multiprocessing.Pool(maxtasksperchild=5,\n",
    "                                         processes=multiprocessing.cpu_count())\n",
    "\n",
    "    def call(self,*args, **kwargs):\n",
    "        self.pool.apply_async(self.func, args, kwargs, self.cb_func)\n",
    "\n",
    "    def wait(self):\n",
    "        self.pool.close()\n",
    "        self.pool.join()\n",
    "\n",
    "def cb_func(f):\n",
    "    print(\"PID: %d \\t Value: %s completed\" % (os.getpid(), f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineXRT(name, outpath):\n",
    "    ''' This combines the individual observations\n",
    "    \n",
    "    IMPORTANT! The scripts that this (and other functions) creat are designed\n",
    "    to be run from the same directory as this notebook. They WILL NOT work \n",
    "    if you try to run them from the individual data directories.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # find the x-ray files\n",
    "    files = glob(f'{outpath}/{name}/**/*xpcw3po_cl.evt.gz', recursive=True)\n",
    "\n",
    "    if len(files) < 1:\n",
    "        return\n",
    "\n",
    "    # write xsel.in\n",
    "    with open(f'{outpath}/{name}/{name}_xsel.in', 'w') as f:\n",
    "        for i, f_in in enumerate(files):\n",
    "            f_parts = f_in.split('/')\n",
    "            if i == 0:\n",
    "                f.writelines('xsel\\n')\n",
    "                f.writelines('read events\\n')\n",
    "                # set the data directory\n",
    "                f.writelines('/'.join(f_parts[:3]) + '\\n')\n",
    "                # first entry\n",
    "                f.writelines('/'.join(f_parts[3:]) + '\\n')\n",
    "                f.writelines('yes\\n')\n",
    "                continue\n",
    "\n",
    "            f.writelines('read events\\n')\n",
    "            f.writelines('/'.join(f_parts[3:]) + '\\n')\n",
    "\n",
    "        f.writelines('extract events\\n')\n",
    "        f.writelines(f'save events {\"/\".join(f_parts[:3])}/{name}_events.fits\\n')\n",
    "        f.writelines('yes\\n')\n",
    "\n",
    "                     \n",
    "        f.writelines('set phaname PI\\n')\n",
    "        # here we are going to make a few binned images for a few different energy ranges\n",
    "        # energies in loop\n",
    "        for eng in [200, 300, 400, 500, 600]:\n",
    "            f.writelines(f'filter pha_cutoff 50 {eng}\\n')\n",
    "\n",
    "            # save non-binned image -- the yes's are to overwrite if file is already there\n",
    "            f.writelines('extract image\\n')\n",
    "            f.writelines(f'save image {\"/\".join(f_parts[:3])}/{name}_img_50-{eng}.fits\\n')\n",
    "            if os.path.isfile(f'{outpath}/{name}/{name}_events.fits'):\n",
    "                f.writelines('yes\\n')\n",
    "\n",
    "            # save binned image -- see above\n",
    "            f.writelines('set xybinsize 8\\n')\n",
    "            f.writelines('extract image\\n')\n",
    "            f.writelines(f'save image {\"/\".join(f_parts[:3])}/{name}_img_50-{eng}_bl8.fits\\n')\n",
    "            if os.path.isfile(f'{outpath}/{name}/{name}_img_50-{eng}_bl8.fits'):\n",
    "                f.writelines('yes\\n')\n",
    "\n",
    "            f.writelines('set xybinsize 4\\n')\n",
    "            f.writelines('extract image\\n')\n",
    "            f.writelines(f'save image {\"/\".join(f_parts[:3])}/{name}_img_50-{eng}_bl4.fits\\n')\n",
    "            if os.path.isfile(f'{outpath}/{name}/{name}_img_50-{eng}_bl4.fits'):\n",
    "                f.writelines('yes\\n')\n",
    "\n",
    "        f.writelines('exit\\n')\n",
    "        f.writelines('no\\n')\n",
    "\n",
    "    # call xselect\n",
    "    os.system(f'xselect < {outpath}/{name}/{name}_xsel.in')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_expmap(name, outpath):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineXRT_exp(name, outpath):\n",
    "    ''' This combines the exposure maps'''\n",
    "    \n",
    "    # find the x-ray files\n",
    "    files = glob(f'{outpath}/{name}/**/*xpc_ex.img.gz', recursive=True)\n",
    "\n",
    "    if len(files) < 1:\n",
    "        return name\n",
    "\n",
    "    # remove the old file if it is there\n",
    "    if os.path.isfile(f'{outpath}/{name}/{name}_exp.fits'):\n",
    "        os.remove(f'{outpath}/{name}/{name}_exp.fits')\n",
    "\n",
    "    # write xsel.in\n",
    "    with open(f'{outpath}/{name}/{name}_ximg_exp.in', 'w') as f:\n",
    "        for i, f_in in enumerate(files):\n",
    "            f_parts = f_in.split('/')\n",
    "            f.writelines(f'read {f_in}\\n')\n",
    "            if i == 0:\n",
    "                continue\n",
    "            f.writelines('sum\\n')\n",
    "            f.writelines('save\\n')\n",
    "\n",
    "\n",
    "        f.writelines(f'write/fits {\"/\".join(f_parts[:3])}/{name}_exp.fits\\n')\n",
    "\n",
    "        f.writelines('exit\\n')\n",
    "\n",
    "    # call ximage\n",
    "    os.system(f'ximage < {outpath}/{name}/{name}_ximg_exp.in')\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_contours(name, outpath):\n",
    "\n",
    "    if not os.path.isfile(f'{outpath}/{name}/{name}_events.fits'):\n",
    "        return\n",
    "\n",
    "    # start ds9\n",
    "    p = subprocess.Popen(['ds9', f'{outpath}/{name}/{name}_events.fits'])\n",
    "\n",
    "    ds9_cmds = ['xpaset -p ds9 bin factor 8',\n",
    "                \"xpaset -p ds9 bin filter 'pi=50:200'\",\n",
    "                'xpaset -p ds9 smooth 3',\n",
    "                'xpaset -p ds9 contour yes',\n",
    "                'xpaset -p ds9 contour smooth 1',\n",
    "                'xpaset -p ds9 contour color white',\n",
    "                'xpaset -p ds9 contour convert',\n",
    "                f'xpaset -p ds9 regions save {outpath}/{name}/{name}_contours.reg',\n",
    "                'xpaset -p ds9 exit']\n",
    "\n",
    "    # wait for ds9 to start\n",
    "    print(\"waiting 2 seconds for ds9 to start\")\n",
    "    sleep(2)\n",
    "\n",
    "    for cmd in ds9_cmds:\n",
    "        os.system(cmd)\n",
    "\n",
    "    p.wait()\n",
    "    p.kill()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_contours2(name, outpath):\n",
    "\n",
    "    if not os.path.isfile(f'{outpath}/{name}/{name}_img_50-200_bl8.fits'):\n",
    "        return\n",
    "\n",
    "    if not os.path.isfile(f'{outpath}/{name}/{name}_img_50-200_bl8.det'):\n",
    "        print('No sources detected -- reverting to old contours')\n",
    "        mk_contours(name)\n",
    "        return\n",
    "\n",
    "    # start ds9\n",
    "    p = subprocess.Popen(['ds9', f'{outpath}/{name}/{name}_img_50-200_bl8.fits'])\n",
    "\n",
    "    # figure out the contour levels\n",
    "    with open(f'{outpath}/{name}/{name}_img_50-200_bl8.det', 'r') as f:\n",
    "        for l in f.readlines():\n",
    "            if 'Back' in l:\n",
    "                background = float(l.split(':')[-1])\n",
    "                break\n",
    "\n",
    "    # now scale the background appropriately -- 5 sigma\n",
    "    cat = fits.getdata(f'{outpath}/{name}/{name}xrt.fits')\n",
    "    exp_time = cat['xrt_exposure'].sum()\n",
    "\n",
    "    # 8x8 binning\n",
    "    min_pixel = background * 8 * 8 * exp_time * 5\n",
    "\n",
    "    # now figure out the top level\n",
    "    img = fits.getdata(f'{outpath}/{name}/{name}_img_50-200_bl8.fits')\n",
    "    # smooth the image\n",
    "    kernal = Gaussian2DKernel(stddev=1)\n",
    "    smoothed_img = convolve(img, kernal)\n",
    "    max_pixel = smoothed_img.max()\n",
    "\n",
    "    # generate contour levels using a sqrt scale\n",
    "    # i got this from the ds9 source code\n",
    "    nlvls = 5\n",
    "    lvls = np.linspace(0, 1, nlvls) / (nlvls - 1)\n",
    "    lvls = lvls**2 * (max_pixel - min_pixel) + min_pixel\n",
    "    lvls_str = [str(i) for i in lvls]\n",
    "    lvls_str = ' '.join(lvls_str)\n",
    "\n",
    "    ds9_cmds = ['xpaset -p ds9 smooth 3',\n",
    "                'xpaset -p ds9 contour yes',\n",
    "                'xpaset -p ds9 contour nlevels 5',\n",
    "                'xpaset -p ds9 contour levels \"{{{}}}\"'.format(lvls_str),\n",
    "                'xpaset -p ds9 contour smooth 1',\n",
    "                'xpaset -p ds9 contour color white',\n",
    "                'xpaset -p ds9 contour convert',\n",
    "                f'xpaset -p ds9 regions save {outpath}/{name}/{name}_contours.reg',\n",
    "                'xpaset -p ds9 exit']\n",
    "\n",
    "    # wait for ds9 to start\n",
    "    print(\"waiting 2 seconds for ds9 to start\")\n",
    "    sleep(2)\n",
    "\n",
    "    for cmd in ds9_cmds:\n",
    "        os.system(cmd)\n",
    "\n",
    "    p.wait()\n",
    "    p.kill()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_contours(name, outpath):\n",
    "\n",
    "    if not os.path.isfile(f'{outpath}/{name}/{name}_PS1stack_i.fits'):\n",
    "        return\n",
    "\n",
    "    if not os.path.isfile(f'{outpath}/{name}/{name}_contours.reg'):\n",
    "        return\n",
    "\n",
    "    cat = fits.getdata(f'{outpath}/{name}/{name}xrt.fits')\n",
    "\n",
    "    exp_time = cat['xrt_exposure'].sum()\n",
    "    text = f'exp time: {exp_time:.2}s'\n",
    "\n",
    "    gc = aplpy.FITSFigure(f'{outpath}/{name}/{name}_PS1stack_i.fits')\n",
    "    gc.show_rgb(f'{outpath}/{name}/{name}irg.tiff')\n",
    "    try:\n",
    "        gc.show_regions(f'{outpath}/{name}/{name}_contours.reg')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # add exposure info\n",
    "    xo, yo = (80, 80)\n",
    "    plt.text(xo + 2, yo + 2, text, color='black', fontsize=18)\n",
    "    plt.text(xo, yo, text, color='white', fontsize=18)\n",
    "\n",
    "    gc.save(f'{outpath}/{name}/{name}_contours.png')\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_contours2(name, outpath):\n",
    "    if not os.path.isfile(f'{outpath}/{name}/{name}_img_50-200_bl8.fits'):\n",
    "        return\n",
    "\n",
    "    if not os.path.isfile(f'{outpath}/{name}/{name}_img_50-200_bl8.det'):\n",
    "\n",
    "        print('No sources detected -- reverting to old contours')\n",
    "        cat = fits.getdata(f'{outpath}/{name}/{name}xrt.fits')\n",
    "\n",
    "        exp_time = cat['xrt_exposure'].sum()\n",
    "        text = f'exp time: {exp_time:.2}s'\n",
    "\n",
    "        gc = aplpy.FITSFigure(f'{outpath}/{name}/{name}_img_50-200_bl8.fits')\n",
    "        gc.show_grayscale()\n",
    "        try:\n",
    "            gc.show_regions(f'{outpath}/{name}/{name}_contours.reg')\n",
    "        except ValueError:\n",
    "            pass\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # add exposure info\n",
    "        xo, yo = (80, 80)\n",
    "        plt.text(xo + 2, yo + 2, text, color='black', fontsize=18)\n",
    "        plt.text(xo, yo, text, color='white', fontsize=18)\n",
    "\n",
    "        gc.save(f'{outpath}/{name}/{name}_XRT_contours.png')\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        return\n",
    "\n",
    "    # figure out the contour levels\n",
    "    with open(f'{outpath}/{name}/{name}_img_50-200_bl8.det', 'r') as f:\n",
    "        for l in f.readlines():\n",
    "            if 'Back' in l:\n",
    "                background = float(l.split(':')[-1])\n",
    "                break\n",
    "\n",
    "    # now scale the background appropriately -- 5 sigma\n",
    "    cat = fits.getdata(f'{outpath}/{name}/{name}xrt.fits')\n",
    "    exp_time = cat['xrt_exposure'].sum()\n",
    "\n",
    "    # 8x8 binning\n",
    "    min_pixel = background * 8 * 8 * exp_time * 5\n",
    "\n",
    "    # now figure out the top level\n",
    "    img = fits.getdata(f'{outpath}/{name}/{name}_img_50-200_bl8.fits')\n",
    "    # smooth the image\n",
    "    kernal = Gaussian2DKernel(stddev=1, x_size=3, y_size=3)\n",
    "    smoothed_img = convolve(img, kernal)\n",
    "    max_pixel = smoothed_img.max()\n",
    "\n",
    "    # generate contour levels using a sqrt scale\n",
    "    # i got this from the ds9 source code\n",
    "    nlvls = 5\n",
    "    lvls = np.linspace(0, 1, nlvls) / (nlvls - 1)\n",
    "    lvls = lvls**2 * (max_pixel - min_pixel) + min_pixel\n",
    "    lvls_str = [str(i) for i in lvls]\n",
    "    lvls_str = ' '.join(lvls_str)\n",
    "\n",
    "    cat = fits.getdata(f'{outpath}/{name}/{name}xrt.fits')\n",
    "\n",
    "    exp_time = cat['xrt_exposure'].sum()\n",
    "    text = f'exp time: {exp_time:.2}s'\n",
    "\n",
    "    gc = aplpy.FITSFigure(f'{outpath}/{name}/{name}_img_50-200_bl8.fits')\n",
    "    gc.show_grayscale()\n",
    "    gc.show_contour(smooth=3, levels=lvls, cmap='viridis')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # add exposure info\n",
    "    xo, yo = (80, 80)\n",
    "    plt.text(xo + 2, yo + 2, text, color='black', fontsize=18)\n",
    "    plt.text(xo, yo, text, color='white', fontsize=18)\n",
    "\n",
    "    gc.save(f'{outpath}/{name}/{name}_XRT_contours.png')\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_detec(name, outpath):\n",
    "    if not os.path.isfile(f'{outpath}/{name}/{name}_img_50-200_bl8.fits'):\n",
    "        return\n",
    "\n",
    "    with open(f'{outpath}/{name}/{name}_ximg.in', 'w') as f:\n",
    "        f.writelines(f'read {outpath}/{name}/{name}_img_50-200_bl8.fits\\n')\n",
    "\n",
    "        f.writelines('detect/snr_threshold=3/'\n",
    "                f'fitsdet={{{outpath}/{name}/{name}_img_50-200_bl8.det.fits}}\\n')\n",
    "        f.writelines('detect/snr_threshold=3/'\n",
    "                f'filedet={{{outpath}/{name}/{name}_img_50-200_bl8.det}}\\n')\n",
    "        f.writelines('exit\\n')\n",
    "\n",
    "    # remove old file if it exists\n",
    "    if os.path.isfile(f'{outpath}/{name}/{name}_img_50-200_bl8.det.fits'):\n",
    "        os.remove(f'{outpath}/{name}/{name}_img_50-200_bl8.det.fits')\n",
    "\n",
    "    # call xselect\n",
    "    os.system(f'ximage < {outpath}/{name}/{name}_ximg.in')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_PSZcatalog():\n",
    "    from astropy.table import Table                                                       \n",
    "    from numpy import append as npappend                                             \n",
    "\n",
    "    datapath = './../planckClusters/catalogs/'\n",
    "    \n",
    "    ps1 = Table.read(f'{datapath}/PSZ1v2.1.fits')\n",
    "    ps2 = Table.read(f'{datapath}/PSZ2v1.fits')\n",
    "\n",
    "    # convert to pandas\n",
    "    df1 = ps1.to_pandas()\n",
    "    df2 = ps2.to_pandas()\n",
    "\n",
    "    # clean up strings -- not required\n",
    "    df1 = df1.applymap(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
    "    df2 = df2.applymap(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
    "\n",
    "    # merge the catalogs together\n",
    "    df_m = df1.merge(df2, how='outer', left_on='INDEX', right_on='PSZ', suffixes=('_PSZ1', '_PSZ2'))\n",
    "    \n",
    "    # get the columns that we want\n",
    "    cols = df_m.columns[[0, 1, 4, 5, 8, 29, 33, 34, 37, 38, 40, 51]]\n",
    "    df_final = df_m[cols]\n",
    "\n",
    "    # remerge to find bits that were missing                                        \n",
    "    df_final_bigger = df_final.merge(df2, how='left', left_on='INDEX_PSZ1',         \n",
    "                                 right_on='PSZ')\n",
    "    # fill in nans                                                                  \n",
    "    for col in ['NAME', 'RA', 'DEC', 'SNR', 'REDSHIFT', 'INDEX']:                   \n",
    "        df_final_bigger[col+'_PSZ2'] = df_final_bigger[col+'_PSZ2'].fillna(df_final_bigger[col])\n",
    "    # fill in nans                                                                  \n",
    "    for col in ['NAME', 'RA', 'DEC', 'SNR', 'REDSHIFT', 'INDEX']:\n",
    "        df_final_bigger[col+'_PSZ2'] = df_final_bigger[col+'_PSZ2'].fillna(df_final_bigger[col])\n",
    "    for col in ['NAME', 'RA', 'DEC']:\n",
    "        df_final_bigger[col] = df_final_bigger[col+'_PSZ2'].fillna(df_final_bigger[col+'_PSZ1'])\n",
    "\n",
    "    df_final_bigger = df_final_bigger[npappend(df_final_bigger.columns[:12].values, ['NAME', 'RA', 'DEC'])]\n",
    "\n",
    "    return df_final_bigger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: UnitsWarning: 'erg/s/cm2' contains multiple slashes, which is discouraged by the FITS standard [astropy.units.format.generic]\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "\n",
    "# get file data\n",
    "data = load_PSZcatalog()\n",
    "data = data.sort_index(axis=1)\n",
    "\n",
    "outpath = './data_full'\n",
    "\n",
    "# this is the multitasking bit\n",
    "#async_worker = AsyncFactory(combineXRT_exp, cb_func)\n",
    "\n",
    "for i, (ra, dec, name) in enumerate(zip(data['RA'], data['DEC'], data['NAME'])):\n",
    "\n",
    "    #print(name)\n",
    "    name = name.replace(' ', '_')\n",
    "\n",
    "    combineXRT(name, outpath)\n",
    "    #async_worker.call(name, outpath)\n",
    "    #combineXRT_exp(name, outpath)\n",
    "    #source_detec(name, outpath)\n",
    "    #mk_contours(name)\n",
    "    #mk_contours2(name)\n",
    "    #put_contours(name)\n",
    "    #put_contours2(name)\n",
    "\n",
    "#async_worker.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

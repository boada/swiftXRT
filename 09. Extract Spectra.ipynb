{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(f'{os.environ[\"HOME\"]}/Projects/planckClusters/catalogs')\n",
    "from load_catalogs import load_PSZcatalog\n",
    "from astropy.table import Table\n",
    "from astropy.io import fits\n",
    "from astropy import wcs\n",
    "from glob import glob\n",
    "import shlex\n",
    "from regions import read_ds9, write_ds9\n",
    "import numpy.ma as ma\n",
    "\n",
    "# parallel processor et al.\n",
    "from utilities import parallel_process, check_exe, system_call, system_call_env\n",
    "from utilities import get_immediate_subdirectories, redshifts_from_papers\n",
    "from model import inv_beta_model\n",
    "\n",
    "import warnings\n",
    "from astropy.utils.exceptions import AstropyWarning\n",
    "warnings.simplefilter('ignore', category=AstropyWarning)\n",
    "from astropy import log\n",
    "log.setLevel('WARN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this allows us to run the pipeline not interactively. \n",
    "os.environ['HEADASNOQUERY'] = ''\n",
    "os.environ['HEADASPROMPT'] = '/dev/null'\n",
    "# os.environ['PFILES'] = '/tmp/$$.tmp/pfiles;$HEADAS/syspfiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runnh(ra, dec):\n",
    "    '''This function will automatically run the ftool nh\n",
    "    on a given RA and Dec in J2000 equinox coordinates'''\n",
    "\n",
    "    nh = 0\n",
    "    cmd = f'nh equinox=2000 ra={ra} dec={dec}'\n",
    "    args = shlex.split(cmd)\n",
    "    out = \"\"\n",
    "\n",
    "    with subprocess.Popen(args,\n",
    "                          stdout=subprocess.PIPE,\n",
    "                          universal_newlines=True) as proc:\n",
    "\n",
    "        proc.wait()\n",
    "        out = proc.stdout.read()\n",
    "\n",
    "    outlines = out.split(\"\\n\")\n",
    "    for line in outlines:\n",
    "        if \"Weighted average\" in line:\n",
    "            nh = line.split(\" \")[-1]\n",
    "\n",
    "    return out, float(nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_exptime(name, outpath, base_file, regfile, outfile):\n",
    "    expomap = base_file.replace('cl.evt', 'ex.img')\n",
    "\n",
    "    # load the data\n",
    "    expmap_data = fits.getdata(expomap)\n",
    "\n",
    "    # combine the regions with the annulus.\n",
    "    coords = read_ds9(regfile, errors='warn')\n",
    "    m = coords[0].to_mask()\n",
    "    compound_region = m.to_image(expmap_data.shape)\n",
    "    final_regmask_inv = np.where(compound_region == 0, 1, 0) # this is the final mask for the data\n",
    "\n",
    "    expmap_data_masked = ma.masked_array(expmap_data, mask=final_regmask_inv)\n",
    "\n",
    "\n",
    "    with fits.open(outfile, 'update') as f:\n",
    "        for hdu in f:\n",
    "            try:\n",
    "                hdu.header['EXPOSURE'] = expmap_data_masked.mean()\n",
    "            except KeyError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bkg_regions(name, outpath):\n",
    "    ''' For each obs id we are going to create a background region. 10.5 arcmin\n",
    "    centered on the pointing position of each observation.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # if there aren't detections, don't bother doing anything.\n",
    "    if os.path.isfile(f'{outpath}/{name}/{name}_vtp.detect'):\n",
    "        detects = Table.read(f'{outpath}/{name}/{name}_vtp.detect', hdu=1)\n",
    "    else:\n",
    "        return\n",
    "#         raise FileNotFoundError(f'{outpath}/{name}/{name}_vtp.detect')\n",
    "\n",
    "    # find the event files\n",
    "    files = glob(f'{outpath}/{name}/reduced/**/*xpcw[1-4]po_cl.evt',\n",
    "                 recursive=True)\n",
    "\n",
    "    if len(files) < 1:\n",
    "        return\n",
    "\n",
    "    # create a place for the products\n",
    "    if not os.path.isdir(f'{outpath}/{name}/spec_files'):\n",
    "        os.makedirs(f'{outpath}/{name}/spec_files')\n",
    "\n",
    "    # get the MCMC fits:\n",
    "    if os.path.isfile(f'{outpath}/{name}/{name}_mcmcfits.txt'):\n",
    "        mcmcfit = Table.read(f'{outpath}/{name}/{name}_mcmcfits.txt',\n",
    "                             format='ascii',\n",
    "                             header_start=0)\n",
    "    else:\n",
    "        return    \n",
    "    \n",
    "    # write one background region for each observation\n",
    "    for f_in in files:\n",
    "\n",
    "        # get pointing\n",
    "        point_ra = fits.getheader(f_in, ext=0)['RA_PNT']\n",
    "        point_dec = fits.getheader(f_in, ext=0)['DEC_PNT']\n",
    "\n",
    "        # we cannot use the X/Y position in the catalog because it was detected \n",
    "        # in the combined frame. We need to use the RA/DEC and then find the \n",
    "        # pixel positions in the individual frames. Lame.\n",
    "        # WCS info to convert world coords to pixels\n",
    "        hdr = fits.getheader(f_in.replace('cl.evt', 'ex.img'))\n",
    "        w = wcs.WCS(hdr)\n",
    "        pixels = w.wcs_world2pix(point_ra, point_dec, 0)\n",
    "\n",
    "        obs_id = f_in.split('/')[-2]\n",
    "\n",
    "        # write out the regions\n",
    "        with open(f'{outpath}/{name}/spec_files/{obs_id}_bkg.reg', 'w') as reg:\n",
    "            reg.write('image\\n')\n",
    "            # write the background region -- different sizes for different types of obs.\n",
    "            if f_in[-10] == '3':\n",
    "                reg.write(\n",
    "                    f\"circle({pixels[0]:.5f},{pixels[1]:.5f},{(11 * 60 /2.36):.5f})\\n\"\n",
    "                )\n",
    "            else:\n",
    "                reg.write(\n",
    "                    f\"circle({pixels[0]:.5f},{pixels[1]:.5f},{(9.5 * 60 /2.36):.5f})\\n\"\n",
    "                )\n",
    "\n",
    "            for j, xc, yc, rc, rotc in detects[['INDEX', 'RA', 'DEC', 'R', 'ROTANG']]:\n",
    "                \n",
    "                # gotta work in pixel coordinates\n",
    "                pixels = w.wcs_world2pix(xc, yc, 0) \n",
    "                \n",
    "                # we didn't fit all the sources with the MCMC -- use the VTP region\n",
    "                if j not in mcmcfit['ID']:\n",
    "                    reg.write(f'-ellipse({pixels[0]:.5f},{pixels[1]:.5f},{rc[0]:.3f}'\n",
    "                              f',{rc[1]:.3f},{rotc:.3f})\\n')\n",
    "                    continue\n",
    "                \n",
    "                # Here we are going to draw a circle with a radius where the beta \n",
    "                # model just fades into the background. These values come from \n",
    "                # the MCMC fits done previously.\n",
    "                radius = inv_beta_model(mcmcfit['So_50'][mcmcfit['ID'] == j],\n",
    "                                        mcmcfit['rc_50'][mcmcfit['ID'] == j],\n",
    "                                        mcmcfit['beta_50'][mcmcfit['ID'] == j],\n",
    "                                        mcmcfit['bg_50'][mcmcfit['ID'] == j])               \n",
    "                \n",
    "                reg.write(\n",
    "                    f\"-circle({pixels[0]}, {pixels[1]}, {(1.5 * radius[0] * 60 / 2.36):.3f})\\n\"\n",
    "                )\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_src_regions(name, outpath):\n",
    "    ''' For each obs id we are going to create a background region. 10.5 arcmin\n",
    "    centered on the pointing position of each observation.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # if there aren't detections, don't bother doing anything.\n",
    "    if os.path.isfile(f'{outpath}/{name}/{name}_vtp.detect'):\n",
    "        detects = Table.read(f'{outpath}/{name}/{name}_vtp.detect', hdu=1)\n",
    "    else:\n",
    "        return\n",
    "#         raise FileNotFoundError(f'{outpath}/{name}/{name}_vtp.detect')\n",
    "\n",
    "    # find the event files\n",
    "    files = glob(f'{outpath}/{name}/reduced/**/*xpcw[1-4]po_cl.evt',\n",
    "                 recursive=True)\n",
    "\n",
    "    if len(files) < 1:\n",
    "        return\n",
    "\n",
    "    # create a place for the products\n",
    "    if not os.path.isdir(f'{outpath}/{name}/spec_files'):\n",
    "        os.makedirs(f'{outpath}/{name}/spec_files')\n",
    "\n",
    "    # get the MCMC fits:\n",
    "    if os.path.isfile(f'{outpath}/{name}/{name}_mcmcfits.txt'):\n",
    "        mcmcfit = Table.read(f'{outpath}/{name}/{name}_mcmcfits.txt',\n",
    "                             format='ascii',\n",
    "                             header_start=0)\n",
    "    else:\n",
    "        return\n",
    "#         raise FileNotFoundError(f'{outpath}/{name}/{name}_mcmcfits.txt')\n",
    "\n",
    "    # write one background region for each observation\n",
    "    for f_in in files:\n",
    "\n",
    "        obs_id = f_in.split('/')[-2]\n",
    "\n",
    "        # we cannot use the X/Y position in the catalog because it was detected \n",
    "        # in the combined frame. We need to use the RA/DEC and then find the \n",
    "        # pixel positions in the individual frames. Lame.\n",
    "        # WCS info to convert world coords to pixels\n",
    "        hdr = fits.getheader(f_in.replace('cl.evt', 'ex.img'))\n",
    "        w = wcs.WCS(hdr)\n",
    "\n",
    "        # now we have to loop through the detections\n",
    "        for j, xc, yc, ext in detects[['INDEX', 'RA', 'DEC', 'Extended']]:\n",
    "\n",
    "            # we didn't fit all the sources with the MCMC\n",
    "            if j not in mcmcfit['ID']:\n",
    "                continue\n",
    "            elif ext < 1:\n",
    "                continue\n",
    "                \n",
    "            # write out the regions\n",
    "            with open(f'{outpath}/{name}/spec_files/{obs_id}_{j}.reg',\n",
    "                      'w') as reg:\n",
    "                reg.write('image\\n')\n",
    "                \n",
    "                # Here we are going to draw a circle with a radius where the beta \n",
    "                # model just fades into the background. These values come from \n",
    "                # the MCMC fits done previously.\n",
    "                radius = inv_beta_model(mcmcfit['So_50'][mcmcfit['ID'] == j],\n",
    "                                        mcmcfit['rc_50'][mcmcfit['ID'] == j],\n",
    "                                        mcmcfit['beta_50'][mcmcfit['ID'] == j],\n",
    "                                        mcmcfit['bg_50'][mcmcfit['ID'] == j])\n",
    "\n",
    "                pixels = w.wcs_world2pix(xc, yc, 0)\n",
    "\n",
    "                reg.write(\n",
    "                    f\"circle({pixels[0]}, {pixels[1]}, {(radius[0] * 60 / 2.36):.3f})\\n\"\n",
    "                )\n",
    "\n",
    "                # have to loop through them again for the masks\n",
    "                for k, xc2, yc2, rc, rotc in detects[['INDEX', 'RA', 'DEC', 'R', 'ROTANG']]:\n",
    "                    \n",
    "                    if not j == k:\n",
    "                        \n",
    "                        pixels = w.wcs_world2pix(xc2, yc2, 0)\n",
    "                        \n",
    "                        # we didn't fit all the sources with the MCMC\n",
    "                        if k not in mcmcfit['ID']:\n",
    "                            reg.write(f'-ellipse({pixels[0]:.5f},{pixels[1]:.5f},'\n",
    "                                  f'{rc[0]:.3f},{rc[1]:.3f},{rotc:.3f})\\n')\n",
    "                            continue\n",
    "                        \n",
    "                        # Here we are going to draw a circle with a radius where the beta \n",
    "                        # model just fades into the background. These values come from \n",
    "                        # the MCMC fits done previously.\n",
    "                        radius = inv_beta_model(mcmcfit['So_50'][mcmcfit['ID'] == k],\n",
    "                                                mcmcfit['rc_50'][mcmcfit['ID'] == k],\n",
    "                                                mcmcfit['beta_50'][mcmcfit['ID'] == k],\n",
    "                                                mcmcfit['bg_50'][mcmcfit['ID'] == k])\n",
    "\n",
    "                        reg.write(\n",
    "                            f\"-circle({pixels[0]}, {pixels[1]}, {(1.5 * radius[0] * 60 / 2.36):.3f})\\n\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xselect_pha(name, outpath):\n",
    "    # if there aren't detections, don't bother doing anything.\n",
    "    if os.path.isfile(f'{outpath}/{name}/{name}_vtp.detect'):\n",
    "        detects = Table.read(f'{outpath}/{name}/{name}_vtp.detect', hdu=1)\n",
    "    else:\n",
    "        return\n",
    "#         raise FileNotFoundError(f'{outpath}/{name}/{name}_vtp.detect')\n",
    "\n",
    "    # find the event files\n",
    "    files = glob(f'{outpath}/{name}/reduced/**/*xpcw[1-4]po_cl.evt',\n",
    "                 recursive=True)\n",
    "\n",
    "    if len(files) < 1:\n",
    "        return\n",
    "\n",
    "    # write one background region for each observation\n",
    "    for f_in in files:\n",
    "\n",
    "        obs_id = f_in.split('/')[-2]\n",
    "\n",
    "        # create the background xselect call first\n",
    "        if not os.path.isfile(f'{outpath}/{name}/spec_files/{obs_id}_bkg.reg'):\n",
    "            continue\n",
    "\n",
    "        # write background xsel.in\n",
    "        with open(f'{outpath}/{name}/spec_files/{obs_id}_bkg.in', 'w') as f:\n",
    "            # this is the session name... specify random letters to run many at once.\n",
    "            f.writelines(f'{obs_id}\\n')\n",
    "            f.writelines('read events\\n')\n",
    "            # set the data directory\n",
    "            f.writelines('/'.join(f_in.split('/')[:5]) + '\\n')\n",
    "            # first entry\n",
    "            f.writelines('/'.join(f_in.split('/')[5:]) + '\\n')\n",
    "            f.writelines('yes\\n')\n",
    "            f.writelines(\n",
    "                f'filter region {outpath}/{name}/spec_files/{obs_id}_bkg.reg\\n'\n",
    "            )\n",
    "            f.writelines('extract spectrum\\n')\n",
    "            f.writelines(\n",
    "                f'save spectrum {outpath}/{name}/spec_files/{obs_id}_bkg_pc.pha\\n'\n",
    "            )\n",
    "            if os.path.isfile(\n",
    "                    f'{outpath}/{name}/spec_files/{obs_id}_bkg_pc.pha'):\n",
    "                f.writelines('yes\\n')\n",
    "            f.writelines('exit\\n')\n",
    "            f.writelines('no\\n')\n",
    "\n",
    "            \n",
    "        # log the output\n",
    "        log_file = f'{outpath}/{name}/spec_files/{obs_id}_bkg_pha.log'   \n",
    "        \n",
    "        # call\n",
    "        os.system(f'xselect @{outpath}/{name}/spec_files/{obs_id}_bkg.in > {log_file}')\n",
    "#         stdout, stderr = system_call_env(f'xselect @{outpath}/{name}/spec_files/{obs_id}_bkg.in')\n",
    "#         with open(log_file, 'w') as f:\n",
    "#             f.writelines(stdout)\n",
    "\n",
    "        update_exptime(name, outpath, f_in,\n",
    "                      f'{outpath}/{name}/spec_files/{obs_id}_bkg.reg',\n",
    "                      f'{outpath}/{name}/spec_files/{obs_id}_bkg_pc.pha')\n",
    "            \n",
    "        # now we do all the sources.\n",
    "        for j in detects['INDEX']:\n",
    "            if not os.path.isfile(\n",
    "                    f'{outpath}/{name}/spec_files/{obs_id}_{j}.reg'):\n",
    "                continue\n",
    "\n",
    "            # write source xsel.in\n",
    "            with open(f'{outpath}/{name}/spec_files/{obs_id}_{j}.in',\n",
    "                      'w') as f:\n",
    "                # this is the session name... specify random letters to run many at once.\n",
    "                f.writelines(f'{obs_id}_{j}\\n')\n",
    "                f.writelines('read events\\n')\n",
    "                # set the data directory\n",
    "                f.writelines('/'.join(f_in.split('/')[:5]) + '\\n')\n",
    "                # first entry\n",
    "                f.writelines('/'.join(f_in.split('/')[5:]) + '\\n')\n",
    "                f.writelines('yes\\n')\n",
    "                f.writelines(\n",
    "                    f'filter region {outpath}/{name}/spec_files/{obs_id}_{j}.reg\\n'\n",
    "                )\n",
    "                f.writelines('extract spectrum\\n')\n",
    "                f.writelines(\n",
    "                    f'save spectrum {outpath}/{name}/spec_files/{obs_id}_{j}_pc.pha\\n'\n",
    "                )\n",
    "                if os.path.isfile(\n",
    "                        f'{outpath}/{name}/spec_files/{obs_id}_{j}_pc.pha'):\n",
    "                    f.writelines('yes\\n')\n",
    "                f.writelines('exit\\n')\n",
    "                f.writelines('no\\n')            \n",
    "            \n",
    "#             # call\n",
    "#             stdout, stderr = system_call(f'xselect @{outpath}/{name}/spec_files/{obs_id}_{j}.in')\n",
    "            \n",
    "#             # log the output\n",
    "            log_file = f'{outpath}/{name}/spec_files/{obs_id}_{j}_pha.log'\n",
    "#             with open(log_file, 'w') as f:\n",
    "#                 f.writelines(stdout)\n",
    "                \n",
    "            os.system(f'xselect @{outpath}/{name}/spec_files/{obs_id}_{j}.in > {log_file}')\n",
    "        \n",
    "            update_exptime(name, outpath, f_in,\n",
    "                           f'{outpath}/{name}/spec_files/{obs_id}_{j}.reg',\n",
    "                           f'{outpath}/{name}/spec_files/{obs_id}_{j}_pc.pha')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_arf(name, outpath):\n",
    "    # if there aren't detections, don't bother doing anything.\n",
    "    if os.path.isfile(f'{outpath}/{name}/{name}_vtp.detect'):\n",
    "        detects = Table.read(f'{outpath}/{name}/{name}_vtp.detect', hdu=1)\n",
    "    else:\n",
    "        return\n",
    "#         raise FileNotFoundError(f'{outpath}/{name}/{name}_vtp.detect')\n",
    "\n",
    "    # find the event files\n",
    "    files = glob(f'{outpath}/{name}/reduced/**/**xpcw[1-4]po_ex.img',\n",
    "                 recursive=True)\n",
    "\n",
    "    if len(files) < 1:\n",
    "        return\n",
    "\n",
    "    check_exe('xrtmkarf')\n",
    "\n",
    "    for f_in in files:\n",
    "\n",
    "        obs_id = f_in.split('/')[-2]\n",
    "        # now we have to loop through the detections\n",
    "        for j, ra, dec in detects[['INDEX', 'RA', 'DEC']]:\n",
    "            # check that we have the files\n",
    "            if not os.path.isfile(\n",
    "                    f'{outpath}/{name}/spec_files/{obs_id}_{j}_pc.pha'):\n",
    "                continue\n",
    "            else:\n",
    "                pha = f'{outpath}/{name}/spec_files/{obs_id}_{j}_pc.pha'\n",
    "\n",
    "                \n",
    "            # check to make sure the spectrum has counts    \n",
    "            with open(f'{outpath}/{name}/spec_files/{obs_id}_{j}_pha.log') as logfile:\n",
    "                outlines = logfile.readlines()\n",
    "                for line in outlines:\n",
    "                    line = \" \".join(line.split())\n",
    "                    if \"Spectrum has\" in line:\n",
    "                        counts = int(line.split()[2])\n",
    "                        break\n",
    "\n",
    "            if not counts > 0:\n",
    "                continue\n",
    "                \n",
    "            # we cannot use the X/Y position in the catalog because it was \n",
    "            # detected in the combined frame. We need to use the RA/DEC and then\n",
    "            # find the pixel positions in the individual frames. Lame.\n",
    "            # WCS info to convert world coords to pixels\n",
    "            hdr = fits.getheader(f_in)\n",
    "            w = wcs.WCS(hdr)\n",
    "            pixels = w.wcs_world2pix(ra, dec, 0)\n",
    "\n",
    "            cmd = (f'xrtmkarf phafile={pha} srcx={pixels[0]} srcy={pixels[1]} '\n",
    "                   f'outfile={outpath}/{name}/spec_files/{obs_id}_{j}_pc.arf '\n",
    "                   f'psfflag=yes extended=yes expofile={f_in} clobber+')\n",
    "\n",
    "#             # call\n",
    "#             stdout, stderr = system_call(cmd)\n",
    "            \n",
    "#             # log the output\n",
    "            log_file = f'{outpath}/{name}/spec_files/{obs_id}_{j}_arf.log'\n",
    "#             with open(log_file, 'w') as f:\n",
    "#                 f.writelines(stdout)\n",
    "\n",
    "            os.system(f'{cmd} > {log_file}')    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_spectra(name, outpath, cnts=10):\n",
    "    # if there aren't detections, don't bother doing anything.\n",
    "    if os.path.isfile(f'{outpath}/{name}/{name}_vtp.detect'):\n",
    "        detects = Table.read(f'{outpath}/{name}/{name}_vtp.detect', hdu=1)\n",
    "    else:\n",
    "        return\n",
    "#         raise FileNotFoundError(f'{outpath}/{name}/{name}_vtp.detect')\n",
    "\n",
    "    # get a list of the reduced obs.\n",
    "    reduc = get_immediate_subdirectories(f'{outpath}/{name}/reduced')\n",
    "    reduc_ids = [r.split('/')[-1] for r in reduc]\n",
    "\n",
    "    if len(reduc) < 1:\n",
    "        return\n",
    "\n",
    "    check_exe('addascaspec')\n",
    "    # we are gonna need to change directories\n",
    "    wd = os.getcwd()\n",
    "\n",
    "    # now we do all the sources.\n",
    "    for j, ext in detects[['INDEX', 'Extended']]:\n",
    "\n",
    "        if ext < 1:\n",
    "            continue\n",
    "        \n",
    "        # remove files if they are already there.\n",
    "        if os.path.isfile(f'{outpath}/{name}/spec_files/{j}_pc.pha'):\n",
    "            os.remove(f'{outpath}/{name}/spec_files/{j}_pc.pha')\n",
    "        if os.path.isfile(f'{outpath}/{name}/spec_files/{j}_pc.arf'):\n",
    "            os.remove(f'{outpath}/{name}/spec_files/{j}_pc.arf')\n",
    "        if os.path.isfile(f'{outpath}/{name}/spec_files/{j}_bkg.pha'):\n",
    "            os.remove(f'{outpath}/{name}/spec_files/{j}_bkg.pha')\n",
    "        \n",
    "        good_reduc_ids = [obs_id for obs_id in reduc_ids \n",
    "                         if os.path.isfile(f'{outpath}/{name}/spec_files/{obs_id}_{j}.reg')]\n",
    "\n",
    "        \n",
    "        # here we are going to sort the good reduc ids by exposure time.\n",
    "        # this is because we can't use all of them if there are more than\n",
    "        # ~15 because addascaspec crashes\n",
    "        exp_times = []\n",
    "        for obs_id in good_reduc_ids:\n",
    "            if os.path.isfile(f'{outpath}/{name}/spec_files/{obs_id}_{j}_pc.arf'):\n",
    "                exp_times.append(fits.getheader(f'{outpath}/{name}/spec_files/{obs_id}_{j}_pc.pha')['EXPOSURE'])\n",
    "            else:\n",
    "                exp_times.append(-1.0)\n",
    "        \n",
    "        # sorted exp_times -- high to low\n",
    "        exp_times_argsort = np.argsort(exp_times)[::-1]\n",
    "        \n",
    "        # sort good_reduc_ids based on exp_times -- high to low\n",
    "        good_reduc_ids = [good_reduc_ids[i] for i in exp_times_argsort]\n",
    "        \n",
    "        # build the file lists:\n",
    "        spectra = [f'{obs_id}_{j}_pc.pha' for obs_id in good_reduc_ids\n",
    "                  if os.path.isfile(f'{outpath}/{name}/spec_files/{obs_id}_{j}_pc.arf')]\n",
    "        bkgs = [f'{obs_id}_bkg_pc.pha' for obs_id in good_reduc_ids\n",
    "               if os.path.isfile(f'{outpath}/{name}/spec_files/{obs_id}_{j}_pc.arf')]\n",
    "        arfs = [f'{obs_id}_{j}_pc.arf' for obs_id in good_reduc_ids\n",
    "               if os.path.isfile(f'{outpath}/{name}/spec_files/{obs_id}_{j}_pc.arf')]\n",
    "\n",
    "        # the rmfs are harder to build\n",
    "        rmfs = []\n",
    "        for obs_id in good_reduc_ids:\n",
    "            if os.path.isfile(f'{outpath}/{name}/spec_files/{obs_id}_{j}_pc.arf'):\n",
    "                with open(f'{outpath}/{name}/spec_files/{obs_id}_{j}_arf.log',\n",
    "                          'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    for l in lines:\n",
    "                        # DO NOT REMOVE THE DOUBLE SLASH!!!\n",
    "                        if '/opt/caldb//data/swift/xrt/cpf/rmf/' in l:\n",
    "                            rmfs.append(l.split(\"'\")[1])\n",
    "\n",
    "        if len(spectra) > 0:\n",
    "        \n",
    "            # have to write the file list to pass to the spectrum combiner.\n",
    "            # addascaspec fails when adding too many files together\n",
    "            with open(f'{outpath}/{name}/spec_files/{j}.list', 'w') as f:\n",
    "                f.writelines(f'{\" \".join(spectra[:20])}\\n')\n",
    "                f.writelines(f'{\" \".join(bkgs[:20])}\\n')\n",
    "                f.writelines(f'{\" \".join(arfs[:20])}\\n')\n",
    "                f.writelines(f'{\" \".join(rmfs[:20])}\\n')\n",
    "\n",
    "            cmd = f'addascaspec {j}.list {j}_pc.pha {j}_pc.arf {j}_bkg.pha'\n",
    "\n",
    "            # call\n",
    "            os.chdir(f'{outpath}/{name}/spec_files')\n",
    "#             stdout, stderr = system_call(cmd)                   \n",
    "                             \n",
    "#             # log\n",
    "            log_file = f'{j}_addascaspec.log'\n",
    "#             with open(log_file, 'w') as f:\n",
    "#                 f.writelines(stdout)\n",
    "\n",
    "            os.system(f'{cmd} > {log_file}')    \n",
    "                \n",
    "        # group the counts\n",
    "        cmd = (f\"grppha infile='{j}_pc.pha' outfile='{j}_pc_{cnts}cts.pha' \"\n",
    "               f\"chatter=0 comm='bad 1-31 & group min {cnts} & exit' clob+\")\n",
    "        \n",
    "        if os.path.isfile(f'{j}_pc.pha'):                 \n",
    "                         \n",
    "#             # call\n",
    "#             stdout, stderr = system_call(cmd)                   \n",
    "\n",
    "#             # log\n",
    "            log_file = f'{j}_grppha.log'\n",
    "#             with open(log_file, 'w') as f:\n",
    "#                 f.writelines(stdout)\n",
    "            \n",
    "            os.system(f'{cmd} > {log_file}')\n",
    "            \n",
    "            \n",
    "        os.chdir(wd)\n",
    "    os.chdir(wd)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_xspec_scripts(name, outpath, kT=5, z=0.1, cnts=10):\n",
    "\n",
    "    # if there aren't detections, don't bother doing anything.\n",
    "    if os.path.isfile(f'{outpath}/{name}/{name}_vtp.detect'):\n",
    "        detects = Table.read(f'{outpath}/{name}/{name}_vtp.detect', hdu=1)\n",
    "    else:\n",
    "        return\n",
    "#         raise FileNotFoundError(f'{outpath}/{name}/{name}_vtp.detect')\n",
    "\n",
    "    if not z > 0:\n",
    "        z = 0.1\n",
    "        \n",
    "    # now we have to loop through the detections\n",
    "    for j, ra, dec in detects[['INDEX', 'RA', 'DEC']]:\n",
    "        if not os.path.isfile(\n",
    "                f'{outpath}/{name}/spec_files/{j}_pc_{cnts}cts.pha'):\n",
    "            continue\n",
    "        else:\n",
    "            source_10cts_pha_name = f'{j}_pc_{cnts}cts.pha'\n",
    "\n",
    "        # get our nH value\n",
    "        nH = runnh(ra, dec)[1]\n",
    "\n",
    "        # here is the text we are going to write\n",
    "        text = [\n",
    "            \"statistic chi\\n\",\n",
    "            f\"data 1:1 {outpath}/{name}/spec_files/{source_10cts_pha_name}\\n\",\n",
    "            f'{outpath}/{name}/spec_files/{j}_pc.arf\\n',\n",
    "            f'{outpath}/{name}/spec_files/{j}_bkg.pha\\n',\n",
    "            \"ignore bad\\n\",\n",
    "            \"ignore 1\\n\",\n",
    "            'method leven 10 0.01\\n',\n",
    "            'abund angr\\n',\n",
    "            'xsect bcmc\\n',\n",
    "            'cosmo 70 0 0.70\\n',\n",
    "            'systematic 0\\n',\n",
    "            \"model  phabs*mekal\\n\",\n",
    "            f\"\\t{nH/1e22}, -1\\n\",\n",
    "            f\"\\t{kT}, -1\\n\",\n",
    "            \"\\t1\\n\",\n",
    "            \"\\t0.3\\n\",\n",
    "            f\"\\t{z}\\n\",\n",
    "            \"\\t1\\n\",\n",
    "            \"\\t0\\n\",\n",
    "            \"fit\\n\",\n",
    "            f'cpd {outpath}/{name}/spec_files/{j}_pc_{cnts}cts.ps/cps\\n',\n",
    "            'setplot energy\\n',\n",
    "            'plot ldata\\n',\n",
    "            'cpd none\\n',\n",
    "            \"error 1.0 7\\n\",\n",
    "            \"newpar 1 0\\n\",\n",
    "            \"flux 0.5 2\\n\",\n",
    "            \"flux 0.1 2.4\\n\",\n",
    "            f\"lum 0.5 2.0 {z}\\n\",\n",
    "            f\"lum 0.1 2.4 {z}\\n\",\n",
    "            f\"data 1:1 {outpath}/{name}/spec_files/{j}_pc.pha\\n\",\n",
    "            f'{outpath}/{name}/spec_files/{j}_pc.arf\\n',\n",
    "            f'{outpath}/{name}/spec_files/{j}_bkg.pha\\n',\n",
    "            \"ignore bad\\n\",\n",
    "            \"ignore **-0.5,2.0-**\\n\",\n",
    "            \"show rates\\n\",\n",
    "            \"exit\\n\"\n",
    "        ]\n",
    "\n",
    "        #Write it to the script\n",
    "        with open(f'{outpath}/{name}/spec_files/{j}_pc_{cnts}cts_xspec.in',\n",
    "                  'w') as script:\n",
    "            for line in text:\n",
    "                script.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xspec_scripts(name, outpath, cnts=10, rerun=True):\n",
    "    # if there aren't detections, don't bother doing anything.\n",
    "    if os.path.isfile(f'{outpath}/{name}/{name}_vtp.detect'):\n",
    "        detects = Table.read(f'{outpath}/{name}/{name}_vtp.detect', hdu=1)\n",
    "    else:\n",
    "        return\n",
    "#         raise FileNotFoundError(f'{outpath}/{name}/{name}_vtp.detect')\n",
    "\n",
    "    results = {}\n",
    "    results['field'] = name\n",
    "    \n",
    "    # now we have to loop through the detections\n",
    "    for j in detects['INDEX']:\n",
    "        if not os.path.isfile(f'{outpath}/{name}/spec_files/{j}_pc_{cnts}cts_xspec.in'):\n",
    "            continue\n",
    "        else:\n",
    "            source_xspec_in = f'{outpath}/{name}/spec_files/{j}_pc_{cnts}cts_xspec.in'\n",
    "\n",
    "        # individual source result\n",
    "        results[j] = {}    \n",
    "        \n",
    "        if rerun:\n",
    "            # build the command and call it\n",
    "            cmd = f\"xspec < {source_xspec_in}\"\n",
    "            stdout, stderr = system_call(cmd, shlexify=False)\n",
    "\n",
    "            # log\n",
    "            log_file = f'{outpath}/{name}/spec_files/{j}_xspec.log'\n",
    "            with open(log_file, 'w') as f:\n",
    "                f.writelines(stdout)\n",
    "        \n",
    "        #Read the fluxes and norms from the stdout\n",
    "        photon_flux = {}\n",
    "        energy_flux = {}\n",
    "        cnt_rate = {}\n",
    "        norms = []\n",
    "        norms_err = {}\n",
    "        chi2 = -1\n",
    "        DOF = -1\n",
    "        lum = {}\n",
    "        \n",
    "        count = 1\n",
    "        if rerun:\n",
    "            outlines = stdout.split('\\n')\n",
    "        else:\n",
    "            with open(f'{outpath}/{name}/spec_files/{j}_xspec.log', 'r') as f:\n",
    "                outlines = f.readlines() \n",
    "                outlines = [line.strip('\\n') for line in outlines]\n",
    "        for line in outlines:\n",
    "            # the thaw line adds an additional chi-square output\n",
    "            # see below\n",
    "            if \"thaw 1\" in line:\n",
    "                count -= 1\n",
    "\n",
    "            # Here is the actual parsing of the log file\n",
    "            if \"Model Flux\" in line:                \n",
    "                band_low = float(line.split()[7].split(\"(\")[1])\n",
    "                band_high = float(line.split()[9])\n",
    "                photons = float(line.split()[2])                \n",
    "                band = f'{band_low:.1f}-{band_high:.1f}' \n",
    "                photon_flux[band] = photons\n",
    "                energy = float(line.split()[4].split(\"(\")[1])\n",
    "                energy_flux[band] = energy\n",
    "                \n",
    "            elif \"2\" in line and \"mekal\" in line and \"norm\" in line:\n",
    "                #the norm will be two index before the +/- sign\n",
    "                pm_index = line.split(\" \").index(\"+/-\")\n",
    "                norms.append(line.split(\" \")[pm_index - 2])\n",
    "#                 norms_err.append(line.split(\" \")[pm_index + 2])\n",
    "            elif \"Net count rate\" in line:\n",
    "                pm_index = line.split(\" \").index(\"+/-\")\n",
    "                cnt_rate['Net'] = line.split(\" \")[pm_index - 1]\n",
    "                cnt_rate['Net_err'] = line.split(\" \")[pm_index + 1]\n",
    "            elif \"Model predicted rate\" in line:\n",
    "                cnt_rate['Model'] = float(line.split(' ')[4])\n",
    "            elif \"Reduced chi-squared\" in line and count <= 2:\n",
    "                chi2 = float(line.split()[3])\n",
    "                DOF = int(line.split()[5])\n",
    "                count += 1\n",
    "            elif \"Model Luminosity\" in line:\n",
    "                band_low = float(line.split()[4].split(\"(\")[1])\n",
    "                band_high = float(line.split()[6])\n",
    "                band = f'{band_low:.1f}-{band_high:.1f}'\n",
    "                lum[band] = float(line.split()[2])\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            #Assuming the current script format, this is the one we want\n",
    "            #This is with PE Absorption\n",
    "            goodnorm = float(norms[1])\n",
    "\n",
    "            #Get the index of the line containing the sigmas\n",
    "            error_index = outlines.index('XSPEC12>error 1.0 7')\n",
    "            sigma_line = outlines[error_index + 2]\n",
    "            sigma_low = float(\n",
    "                sigma_line.split(\" \")[-1].split(\"(\")[1].split(\",\")[0])\n",
    "            sigma_high = float(\n",
    "                sigma_line.split(\" \")[-1].split(\"(\")[1].split(\",\")\n",
    "                [1].split(\")\")[0])\n",
    "            sigma_av = (abs(sigma_low) + abs(sigma_high)) / 2\n",
    "\n",
    "            norms_err['norm_neg'] = abs(sigma_low)\n",
    "            norms_err['norm_pos'] = sigma_high\n",
    "                                         \n",
    "            SN = goodnorm / sigma_av\n",
    "        except (ValueError, IndexError):\n",
    "            goodnorm = -1\n",
    "            SN = -1\n",
    "            norms_err['norm_neg'] = -1\n",
    "            norms_err['norm_pos'] = -1\n",
    "\n",
    "#         # This replaces the commented block above... \n",
    "#         # Assuming the current script format, this is the one we want\n",
    "#         # This is with PE Absorption\n",
    "#         SN = float(norms[1]) / float(norms_err[1])\n",
    "        \n",
    "        results[j]['SN'] = SN\n",
    "        results[j]['Chi2'] = chi2\n",
    "        results[j]['DOF'] = DOF\n",
    "        results[j]['Norm'] = goodnorm\n",
    "        results[j]['Norm_err'] = norms_err\n",
    "        results[j]['flux'] = energy_flux\n",
    "        results[j]['cnt_rate'] = cnt_rate\n",
    "        results[j]['Lum'] = lum\n",
    "\n",
    "    results_file = f'{outpath}/{name}/spec_files/{name}_xspec.results'\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(f'# Spectral Info for {results[\"field\"]}.\\n')\n",
    "        f.write(f'# All fluxes are in the 0.5-2.0 keV band.\\n')\n",
    "        f.write(f'# INDEX SN redCHI2 DOF NORM NORM_NEG NORM_POS '\n",
    "                'FLUX CNT_RATE_NET CNT_RATE_NET_ERR CNT_RATE_MODEL ECF LUM\\n')\n",
    "        \n",
    "        try:\n",
    "            sources = list(results.keys())[1:]\n",
    "        except AttributeError: # it might not be a dict\n",
    "            print(f'{name} Error!')\n",
    "            return results\n",
    "        \n",
    "        if len(sources) < 1:\n",
    "            return results\n",
    "        \n",
    "        for indx in sources:\n",
    "            # basic info first\n",
    "            f.write(f'{indx} {results[indx][\"SN\"]:.4f} {results[indx][\"Chi2\"]:.4f} {results[indx][\"DOF\"]} ')\n",
    "            \n",
    "            # model norms\n",
    "            f.write(f'{results[indx][\"Norm\"]} '\n",
    "                    f'{results[indx][\"Norm_err\"][\"norm_neg\"]} '\n",
    "                    f'{results[indx][\"Norm_err\"][\"norm_pos\"]} ')\n",
    "            \n",
    "            # write the fluxes\n",
    "            try:\n",
    "                flux = results[indx]['flux']['0.5-2.0']\n",
    "#                 flux = results[indx]['flux']['0.1-2.4']\n",
    "                f.write(f'{flux} ')\n",
    "            except KeyError: # no flux or cnt_rate recorded\n",
    "                flux = -1\n",
    "                f.write(f'{flux} ')\n",
    "            \n",
    "            # write out the count rates\n",
    "            for cnt_rate_type in ['Net', 'Net_err', 'Model']:\n",
    "                try:\n",
    "                    cnt_rate = results[indx]['cnt_rate'][cnt_rate_type]\n",
    "                    f.write(f'{cnt_rate} ')\n",
    "                except KeyError:\n",
    "                    cnt_rate = -1\n",
    "                    f.write(f'{cnt_rate} ')\n",
    "            \n",
    "            # ECF\n",
    "            if flux < 0:\n",
    "                f.write(f'{flux} ')\n",
    "            else:\n",
    "                try:\n",
    "                    f.write(f'{flux / results[indx][\"cnt_rate\"][\"Model\"]:.4e} ')\n",
    "                except ZeroDivisionError:\n",
    "                    f.write('-1 ')\n",
    "\n",
    "            # Luminosity\n",
    "            try:\n",
    "                lum = results[indx]['Lum']['0.5-2.0']\n",
    "#                 lum = results[indx]['Lum']['0.1-2.4']\n",
    "                f.write(f'{lum} ')\n",
    "            except KeyError: # no luminosity\n",
    "                lum = -1\n",
    "                f.write(f'{lum} ')    \n",
    " \n",
    "            f.write('\\n')\n",
    "\n",
    "#     print(results)    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file data\n",
    "data = load_PSZcatalog()\n",
    "data = data.sort_values('NAME')\n",
    "\n",
    "data = redshifts_from_papers(data)\n",
    "\n",
    "outpath = './data'\n",
    "\n",
    "arr = [{'name':n.replace(' ', '_'), 'outpath':outpath} for n in data['NAME']]\n",
    "# parallel_process(arr, create_bkg_regions, use_kwargs=True, n_jobs=1)\n",
    "# parallel_process(arr, create_src_regions, use_kwargs=True, n_jobs=1)\n",
    "# parallel_process(arr, xselect_pha, use_kwargs=True, n_jobs=4) # 1.5 hours\n",
    "# parallel_process(arr, mk_arf, use_kwargs=True, n_jobs=4) # 50 minutes\n",
    "# parallel_process(arr, combine_spectra, use_kwargs=True, n_jobs=1)\n",
    "\n",
    "arr = [{'name':n.replace(' ', '_'), 'outpath':outpath, 'z':z} for n, z in zip(data['NAME'], data['REDSHIFT'])]\n",
    "parallel_process(arr, mk_xspec_scripts, use_kwargs=True, n_jobs=1)\n",
    "\n",
    "# arr = [{'name':n.replace(' ', '_'), 'outpath':outpath, 'rerun':False} for n in data['NAME']]\n",
    "# results = parallel_process(arr, run_xspec_scripts, use_kwargs=True, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For testing ###\n",
    "\n",
    "\n",
    "outpath ='./data_full'\n",
    "name = 'PSZ2_G075.08+19.83'\n",
    "# name = 'PSZ2_G139.62+24.18'\n",
    "# name = 'PSZ1_G121.35-42.47'\n",
    "# name = 'PSZ2_G057.80+88.00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cleanedresults[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(r[1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{0.3 / .0938:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecf = []\n",
    "for r in cleanedresults:\n",
    "    try:\n",
    "        sources = list(r.keys())[1:]\n",
    "    except AttributeError: # it's not a dict\n",
    "        continue\n",
    "    if len(sources) < 1:\n",
    "        continue\n",
    "    for indx in sources:\n",
    "        try:\n",
    "            flux = r[indx]['flux']['0.5-2.0']\n",
    "            cnt_rate = r[indx]['cnt_rate']['Model']\n",
    "        except KeyError: # no flux or cnt_rate recorded\n",
    "            continue\n",
    "        ecf.append(flux/cnt_rate)\n",
    "        if flux/cnt_rate * 1e11 >3:\n",
    "            print(r['field'], indx, r[indx], flux/cnt_rate)\n",
    "#         print(flux/cnt_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecf = np.array(ecf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ecf * 1e11, bins=25, range=[2,3])\n",
    "plt.axvline(np.median(ecf) * 1e11)\n",
    "plt.xlabel('ECF [1e-11]')\n",
    "plt.xlim(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

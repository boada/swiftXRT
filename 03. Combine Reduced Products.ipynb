{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from glob import glob\n",
    "#import aplpy\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve\n",
    "from astropy.convolution import Gaussian2DKernel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add extra code to run things in parallel.\n",
    "The combining exposure maps takes a REALLY long time. So I added this bit so I can run it all in parallel.\n",
    "\n",
    "You don't need to run any of this if you don't want to run things in paralle. I'll try to comment out code at the bottom so you can run things in parallel or not depending on what you want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import traceback, functools\n",
    "\n",
    "def error(msg, *args):\n",
    "    multiprocessing.log_to_stderr()\n",
    "    return multiprocessing.get_logger().error(msg, *args)\n",
    "\n",
    "def trace_unhandled_exceptions(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapped_func(*args, **kwargs):\n",
    "        try:\n",
    "            func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            error(traceback.format_exc())\n",
    "            raise\n",
    "\n",
    "    return wrapped_func\n",
    "\n",
    "class AsyncFactory:\n",
    "    def __init__(self, func, cb_func):\n",
    "        self.func = func\n",
    "        self.cb_func = cb_func\n",
    "        self.pool = multiprocessing.Pool(maxtasksperchild=5,\n",
    "                                         processes=multiprocessing.cpu_count())\n",
    "\n",
    "    def call(self,*args, **kwargs):\n",
    "        self.pool.apply_async(self.func, args, kwargs, self.cb_func)\n",
    "\n",
    "    def wait(self):\n",
    "        self.pool.close()\n",
    "        self.pool.join()\n",
    "\n",
    "def cb_func(f):\n",
    "    print(\"PID: %d \\t Value: %s completed\" % (os.getpid(), f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineXRT(name, outpath):\n",
    "    ''' This combines the individual observations\n",
    "    \n",
    "    IMPORTANT! The scripts that this (and other functions) creat are designed\n",
    "    to be run from the same directory as this notebook. They WILL NOT work \n",
    "    if you try to run them from the individual data directories.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # find the x-ray files\n",
    "    files = glob(f'{outpath}/{name}/reduced/**/*xpcw3po_cl.evt', recursive=True)\n",
    "\n",
    "    # sort the files so we can control their input order\n",
    "    # this is used to make sure all of the data products are the same \n",
    "    # when we are combining observations\n",
    "    \n",
    "    files = np.sort(files)\n",
    "    \n",
    "    if len(files) < 1:\n",
    "        return\n",
    "\n",
    "    # write xsel.in\n",
    "    with open(f'{outpath}/{name}/{name}_xsel.in', 'w') as f:\n",
    "        for i, f_in in enumerate(files):\n",
    "            f_parts = f_in.split('/')\n",
    "            if i == 0:\n",
    "                f.writelines('xsel\\n')\n",
    "                f.writelines('read events\\n')\n",
    "                # set the data directory\n",
    "                f.writelines('/'.join(f_parts[:3]) + '\\n')\n",
    "                # first entry\n",
    "                f.writelines('/'.join(f_parts[3:]) + '\\n')\n",
    "                f.writelines('yes\\n')\n",
    "                continue\n",
    "\n",
    "            f.writelines('read events\\n')\n",
    "            f.writelines('/'.join(f_parts[3:]) + '\\n')\n",
    "\n",
    "        f.writelines('extract events\\n')\n",
    "        f.writelines(f'save events {\"/\".join(f_parts[:3])}/{name}_events.fits\\n')\n",
    "        f.writelines('yes\\n')\n",
    "\n",
    "                     \n",
    "        f.writelines('set phaname PI\\n')\n",
    "        # here we are going to make a few binned images for a few different energy ranges\n",
    "        # energies in loop\n",
    "        for eng in [200, 300, 400, 500, 600]:\n",
    "            f.writelines(f'filter pha_cutoff 50 {eng}\\n')\n",
    "\n",
    "            # save non-binned image -- the yes's are to overwrite if file is already there\n",
    "            f.writelines('extract image\\n')\n",
    "            f.writelines(f'save image {\"/\".join(f_parts[:3])}/{name}_img_50-{eng}.fits\\n')\n",
    "            if os.path.isfile(f'{outpath}/{name}/{name}_img_50-{eng}.fits'):\n",
    "                f.writelines('yes\\n')\n",
    "\n",
    "            # save binned image -- see above\n",
    "            f.writelines('set xybinsize 8\\n')\n",
    "            f.writelines('extract image\\n')\n",
    "            f.writelines(f'save image {\"/\".join(f_parts[:3])}/{name}_img_50-{eng}_bl8.fits\\n')\n",
    "            if os.path.isfile(f'{outpath}/{name}/{name}_img_50-{eng}_bl8.fits'):\n",
    "                f.writelines('yes\\n')\n",
    "\n",
    "            f.writelines('set xybinsize 4\\n')\n",
    "            f.writelines('extract image\\n')\n",
    "            f.writelines(f'save image {\"/\".join(f_parts[:3])}/{name}_img_50-{eng}_bl4.fits\\n')\n",
    "            if os.path.isfile(f'{outpath}/{name}/{name}_img_50-{eng}_bl4.fits'):\n",
    "                f.writelines('yes\\n')\n",
    "\n",
    "        f.writelines('exit\\n')\n",
    "        f.writelines('no\\n')\n",
    "\n",
    "    # log the output\n",
    "    log_file = f'{outpath}/{name}/{name}_xsel.log'\n",
    "                         \n",
    "    # call xselect\n",
    "    os.system(f'xselect < {outpath}/{name}/{name}_xsel.in > {log_file}')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineXRT_exp(name, outpath):\n",
    "    ''' This combines the exposure maps'''\n",
    "    \n",
    "    # find the x-ray files\n",
    "    files = glob(f'{outpath}/{name}/reduced/**/*xpcw3po_ex.img', recursive=True)\n",
    "\n",
    "    # sort the observations -- see above\n",
    "    files = np.sort(files)\n",
    "    \n",
    "    if len(files) < 1:\n",
    "        return name\n",
    "\n",
    "    # remove the old file if it is there\n",
    "    if os.path.isfile(f'{outpath}/{name}/{name}_exp.fits'):\n",
    "        os.remove(f'{outpath}/{name}/{name}_exp.fits')\n",
    "\n",
    "    # write xsel.in\n",
    "    with open(f'{outpath}/{name}/{name}_ximg_exp.in', 'w') as f:\n",
    "        for i, f_in in enumerate(files):\n",
    "            f_parts = f_in.split('/')\n",
    "            f.writelines(f'read {f_in}\\n')\n",
    "            if i == 0:\n",
    "                continue\n",
    "            f.writelines('sum\\n')\n",
    "            f.writelines('save\\n')\n",
    "\n",
    "\n",
    "        f.writelines(f'write/fits {\"/\".join(f_parts[:3])}/{name}_exp.fits\\n')\n",
    "\n",
    "        f.writelines('exit\\n')\n",
    " \n",
    "    # log the output\n",
    "    log_file = f'{outpath}/{name}/{name}_ximg_exp.log'\n",
    "    # call ximage\n",
    "    os.system(f'ximage < {outpath}/{name}/{name}_ximg_exp.in > {log_file}')\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_PSZcatalog():\n",
    "    from astropy.table import Table                                                       \n",
    "    from numpy import append as npappend                                             \n",
    "\n",
    "    datapath = './../planckClusters/catalogs/'\n",
    "    \n",
    "    ps1 = Table.read(f'{datapath}/PSZ1v2.1.fits')\n",
    "    ps2 = Table.read(f'{datapath}/PSZ2v1.fits')\n",
    "\n",
    "    # convert to pandas\n",
    "    df1 = ps1.to_pandas()\n",
    "    df2 = ps2.to_pandas()\n",
    "\n",
    "    # clean up strings -- not required\n",
    "    df1 = df1.applymap(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
    "    df2 = df2.applymap(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
    "\n",
    "    # merge the catalogs together\n",
    "    df_m = df1.merge(df2, how='outer', left_on='INDEX', right_on='PSZ', suffixes=('_PSZ1', '_PSZ2'))\n",
    "    \n",
    "    # get the columns that we want\n",
    "    cols = df_m.columns[[0, 1, 4, 5, 8, 29, 33, 34, 37, 38, 40, 51]]\n",
    "    df_final = df_m[cols]\n",
    "\n",
    "    # remerge to find bits that were missing                                        \n",
    "    df_final_bigger = df_final.merge(df2, how='left', left_on='INDEX_PSZ1',         \n",
    "                                 right_on='PSZ')\n",
    "    # fill in nans                                                                  \n",
    "    for col in ['NAME', 'RA', 'DEC', 'SNR', 'REDSHIFT', 'INDEX']:                   \n",
    "        df_final_bigger[col+'_PSZ2'] = df_final_bigger[col+'_PSZ2'].fillna(df_final_bigger[col])\n",
    "    # fill in nans                                                                  \n",
    "    for col in ['NAME', 'RA', 'DEC', 'SNR', 'REDSHIFT', 'INDEX']:\n",
    "        df_final_bigger[col+'_PSZ2'] = df_final_bigger[col+'_PSZ2'].fillna(df_final_bigger[col])\n",
    "    for col in ['NAME', 'RA', 'DEC']:\n",
    "        df_final_bigger[col] = df_final_bigger[col+'_PSZ2'].fillna(df_final_bigger[col+'_PSZ1'])\n",
    "\n",
    "    df_final_bigger = df_final_bigger[npappend(df_final_bigger.columns[:12].values, ['NAME', 'RA', 'DEC'])]\n",
    "\n",
    "    return df_final_bigger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get file data\n",
    "data = load_PSZcatalog()\n",
    "data = data.sort_index(axis=1)\n",
    "\n",
    "outpath = './data_full'\n",
    "\n",
    "# this is the multitasking bit\n",
    "async_worker = AsyncFactory(combineXRT_exp, cb_func)\n",
    "\n",
    "#for i, (ra, dec, name) in enumerate(zip(data['RA'], data['DEC'], data['NAME'])):\n",
    "for ra, dec, name in tqdm_notebook(zip(data['RA'], data['DEC'], data['NAME'])):\n",
    "    \n",
    "    #print(name)\n",
    "    name = name.replace(' ', '_')\n",
    "\n",
    "    #combineXRT(name, outpath)\n",
    "    async_worker.call(name, outpath)\n",
    "    #combineXRT_exp(name, outpath)\n",
    "\n",
    "async_worker.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

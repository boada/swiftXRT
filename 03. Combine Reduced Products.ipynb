{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add extra code to run things in parallel.\n",
    "The combining exposure maps takes a REALLY long time. So I added this bit so I can run it all in parallel.\n",
    "\n",
    "You don't need to run any of this if you don't want to run things in paralle. I'll try to comment out code at the bottom so you can run things in parallel or not depending on what you want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def parallel_process(array, function, n_jobs=None, use_kwargs=False, front_num=0):\n",
    "    \"\"\"\n",
    "        A parallel version of the map function with a progress bar. \n",
    "\n",
    "        Args:\n",
    "            array (array-like): An array to iterate over.\n",
    "            function (function): A python function to apply to the elements of array\n",
    "            n_jobs (int, default=16): The number of cores to use\n",
    "            use_kwargs (boolean, default=False): Whether to consider the elements of array as dictionaries of \n",
    "                keyword arguments to function \n",
    "            front_num (int, default=3): The number of iterations to run serially before kicking off the \n",
    "                parallel job. This can be useful for catching bugs\n",
    "        Returns:\n",
    "            [function(array[0]), function(array[1]), ...]\n",
    "    \"\"\"\n",
    "    #We run the first few iterations serially to catch bugs\n",
    "    if front_num > 0:\n",
    "        front = [function(**a) if use_kwargs else function(a) for a in array[:front_num]]\n",
    "    #If we set n_jobs to 1, just run a list comprehension. This is useful for benchmarking and debugging.\n",
    "    if n_jobs==1:\n",
    "        out = [function(**a) if use_kwargs else function(a) for a in tqdm_notebook(array[front_num:])]\n",
    "#         if front_num:\n",
    "#             return front + out\n",
    "#         else: \n",
    "#             return out\n",
    "    #Assemble the workers\n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as pool:\n",
    "        #Pass the elements of array into function\n",
    "        if use_kwargs:\n",
    "            futures = [pool.submit(function, **a) for a in array[front_num:]]\n",
    "        else:\n",
    "            futures = [pool.submit(function, a) for a in array[front_num:]]\n",
    "        kwargs = {\n",
    "            'total': len(futures),\n",
    "            'unit': 'it',\n",
    "            'unit_scale': True,\n",
    "            'leave': True\n",
    "        }\n",
    "        #Print out the progress as tasks complete\n",
    "        for f in tqdm_notebook(as_completed(futures), **kwargs):\n",
    "            pass\n",
    "#     out = []\n",
    "#     #Get the results from the futures. \n",
    "#     for i, future in tqdm_notebook(enumerate(futures)):\n",
    "#         try:\n",
    "#             out.append(future.result())\n",
    "#         except Exception as e:\n",
    "#             out.append(e)\n",
    "#     if front_num:\n",
    "#         return front + out\n",
    "#     else: \n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineXRT(name, outpath):\n",
    "    ''' This combines the individual observations\n",
    "    \n",
    "    IMPORTANT! The scripts that this (and other functions) creat are designed\n",
    "    to be run from the same directory as this notebook. They WILL NOT work \n",
    "    if you try to run them from the individual data directories.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # find the x-ray files\n",
    "    files = glob(f'{outpath}/{name}/reduced/**/*xpcw3po_cl.evt', recursive=True)\n",
    "\n",
    "    # sort the files so we can control their input order\n",
    "    # this is used to make sure all of the data products are the same \n",
    "    # when we are combining observations\n",
    "    \n",
    "    files = np.sort(files)\n",
    "    \n",
    "    if len(files) < 1:\n",
    "        return\n",
    "\n",
    "    # write xsel.in\n",
    "    with open(f'{outpath}/{name}/{name}_xsel.in', 'w') as f:\n",
    "        for i, f_in in enumerate(files):\n",
    "            f_parts = f_in.split('/')\n",
    "            if i == 0:\n",
    "                # this is the session name... specify random letters to run many at once.\n",
    "                f.writelines(f'{name}\\n')\n",
    "                f.writelines('read events\\n')\n",
    "                # set the data directory\n",
    "                f.writelines('/'.join(f_parts[:3]) + '\\n')\n",
    "                # first entry\n",
    "                f.writelines('/'.join(f_parts[3:]) + '\\n')\n",
    "                f.writelines('yes\\n')\n",
    "                continue\n",
    "\n",
    "            f.writelines('read events\\n')\n",
    "            f.writelines('/'.join(f_parts[3:]) + '\\n')\n",
    "            # if you try to read more than 20 exposures, it says \"more?\"\n",
    "            if i >= 19:\n",
    "                f.writelines('\\n')\n",
    "            if i >= 42:\n",
    "                f.writelines('\\n')\n",
    "            if i >= 65:\n",
    "                f.writelines('\\n')\n",
    "            if i >= 88:\n",
    "                f.writelines('\\n')\n",
    "            if i >= 111:\n",
    "                f.writelines('\\n')\n",
    "            if i >= 134:\n",
    "                f.writelines('\\n')\n",
    "            if i >= 157:\n",
    "                f.writelines('\\n')\n",
    "            if i >= 180:\n",
    "                f.writelines('\\n')\n",
    "            if i >= 203:\n",
    "                f.writelines('\\n')\n",
    "            if i >= 226:\n",
    "                f.writelines('\\n')\n",
    "            if i >= 249:\n",
    "                f.writelines('\\n')\n",
    "        f.writelines('extract events\\n')\n",
    "        f.writelines(f'save events {outpath}/{name}/{name}_events.fits\\n')\n",
    "        if os.path.isfile(f'{outpath}/{name}/{name}_events.fits'):\n",
    "            f.writelines('yes\\n')\n",
    "        f.writelines('yes\\n')\n",
    "\n",
    "                     \n",
    "        f.writelines('set phaname PI\\n')\n",
    "        # here we are going to make a few binned images for a few different energy ranges\n",
    "        # energies in loop\n",
    "        for eng in [200, 300, 400, 500, 600]:\n",
    "            f.writelines(f'filter pha_cutoff 50 {eng}\\n')\n",
    "\n",
    "            # save non-binned image -- the yes's are to overwrite if file is already there\n",
    "            f.writelines('extract image\\n')\n",
    "            f.writelines(f'save image {\"/\".join(f_parts[:3])}/{name}_img_50-{eng}.fits\\n')\n",
    "            if os.path.isfile(f'{outpath}/{name}/{name}_img_50-{eng}.fits'):\n",
    "                f.writelines('yes\\n')\n",
    "\n",
    "            # save binned image -- see above\n",
    "            f.writelines('set xybinsize 8\\n')\n",
    "            f.writelines('extract image\\n')\n",
    "            f.writelines(f'save image {\"/\".join(f_parts[:3])}/{name}_img_50-{eng}_bl8.fits\\n')\n",
    "            if os.path.isfile(f'{outpath}/{name}/{name}_img_50-{eng}_bl8.fits'):\n",
    "                f.writelines('yes\\n')\n",
    "\n",
    "            f.writelines('set xybinsize 4\\n')\n",
    "            f.writelines('extract image\\n')\n",
    "            f.writelines(f'save image {\"/\".join(f_parts[:3])}/{name}_img_50-{eng}_bl4.fits\\n')\n",
    "            if os.path.isfile(f'{outpath}/{name}/{name}_img_50-{eng}_bl4.fits'):\n",
    "                f.writelines('yes\\n')\n",
    "\n",
    "        f.writelines('exit\\n')\n",
    "        f.writelines('no\\n')\n",
    "\n",
    "    # log the output\n",
    "    log_file = f'{outpath}/{name}/{name}_xsel.log'\n",
    "                         \n",
    "    # call xselect\n",
    "    os.system(f'xselect < {outpath}/{name}/{name}_xsel.in > {log_file}')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineXRT_exp(name, outpath):\n",
    "    ''' This combines the exposure maps'''\n",
    "    \n",
    "    # find the x-ray files\n",
    "    files = glob(f'{outpath}/{name}/reduced/**/*xpcw3po_ex.img', recursive=True)\n",
    "\n",
    "    # sort the observations -- see above\n",
    "    files = np.sort(files)\n",
    "    \n",
    "    if len(files) < 1:\n",
    "        return name\n",
    "\n",
    "    # remove the old file if it is there\n",
    "    if os.path.isfile(f'{outpath}/{name}/{name}_exp.fits'):\n",
    "        os.remove(f'{outpath}/{name}/{name}_exp.fits')\n",
    "\n",
    "    # write xsel.in\n",
    "    with open(f'{outpath}/{name}/{name}_ximg_exp.in', 'w') as f:\n",
    "        for i, f_in in enumerate(files):\n",
    "            f_parts = f_in.split('/')\n",
    "            f.writelines(f'read {f_in}\\n')\n",
    "            if i == 0:\n",
    "                continue\n",
    "            f.writelines('sum\\n')\n",
    "            f.writelines('save\\n')\n",
    "\n",
    "\n",
    "        f.writelines(f'write/fits {\"/\".join(f_parts[:3])}/{name}_exp.fits\\n')\n",
    "\n",
    "        f.writelines('exit\\n')\n",
    " \n",
    "    # log the output\n",
    "    log_file = f'{outpath}/{name}/{name}_ximg_exp.log'\n",
    "    # call ximage\n",
    "    os.system(f'ximage < {outpath}/{name}/{name}_ximg_exp.in > {log_file}')\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_PSZcatalog():\n",
    "    from astropy.table import Table                                                       \n",
    "    from numpy import append as npappend                                             \n",
    "\n",
    "    datapath = './../planckClusters/catalogs/'\n",
    "    \n",
    "    ps1 = Table.read(f'{datapath}/PSZ1v2.1.fits')\n",
    "    ps2 = Table.read(f'{datapath}/PSZ2v1.fits')\n",
    "\n",
    "    # convert to pandas\n",
    "    df1 = ps1.to_pandas()\n",
    "    df2 = ps2.to_pandas()\n",
    "\n",
    "    # clean up strings -- not required\n",
    "    df1 = df1.applymap(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
    "    df2 = df2.applymap(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
    "\n",
    "    # merge the catalogs together\n",
    "    df_m = df1.merge(df2, how='outer', left_on='INDEX', right_on='PSZ', suffixes=('_PSZ1', '_PSZ2'))\n",
    "    \n",
    "    # get the columns that we want\n",
    "    cols = df_m.columns[[0, 1, 4, 5, 8, 29, 33, 34, 37, 38, 40, 51]]\n",
    "    df_final = df_m[cols]\n",
    "\n",
    "    # remerge to find bits that were missing                                        \n",
    "    df_final_bigger = df_final.merge(df2, how='left', left_on='INDEX_PSZ1',         \n",
    "                                 right_on='PSZ')\n",
    "    # fill in nans                                                                  \n",
    "    for col in ['NAME', 'RA', 'DEC', 'SNR', 'REDSHIFT', 'INDEX']:                   \n",
    "        df_final_bigger[col+'_PSZ2'] = df_final_bigger[col+'_PSZ2'].fillna(df_final_bigger[col])\n",
    "    # fill in nans                                                                  \n",
    "    for col in ['NAME', 'RA', 'DEC', 'SNR', 'REDSHIFT', 'INDEX']:\n",
    "        df_final_bigger[col+'_PSZ2'] = df_final_bigger[col+'_PSZ2'].fillna(df_final_bigger[col])\n",
    "    for col in ['NAME', 'RA', 'DEC']:\n",
    "        df_final_bigger[col] = df_final_bigger[col+'_PSZ2'].fillna(df_final_bigger[col+'_PSZ1'])\n",
    "\n",
    "    df_final_bigger = df_final_bigger[npappend(df_final_bigger.columns[:12].values, ['NAME', 'RA', 'DEC'])]\n",
    "\n",
    "    return df_final_bigger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: UnitsWarning: 'erg/s/cm2' contains multiple slashes, which is discouraged by the FITS standard [astropy.units.format.generic]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bec8a2f50604284b5e22997c042e91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1943), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get file data\n",
    "data = load_PSZcatalog()\n",
    "data = data.sort_index(axis=1)\n",
    "\n",
    "outpath = './data_full'\n",
    "\n",
    "arr = [{'name':n.replace(' ', '_'), 'outpath':outpath} for n in data['NAME']]\n",
    "parallel_process(arr, combineXRT, use_kwargs=True)\n",
    "#parallel_process(arr, combineXRT_exp, use_kwargs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
